{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe9448a-4428-41aa-b611-12eda5c7126d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# analyze and display monthly max transactions for categories A/B with flag Y from sample data\n",
    "from pyspark.sql.functions import col, avg, max as spark_max, min as spark_min, count, when, lit, year, month, dayofmonth, rand\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "import datetime\n",
    "\n",
    "# 1. Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"date\", DateType(), False)\n",
    "])\n",
    "\n",
    "# 2. Generate sample data\n",
    "categories = [\"A\", \"B\", \"C\", \"D\"]\n",
    "base_date = datetime.date(2025, 1, 1)\n",
    "data = []\n",
    "for i in range(1, 101):\n",
    "    cat = categories[i % 4]\n",
    "    amt = round(1000 * (i % 7) + (i * 3.14) % 500, 2)\n",
    "    dt = base_date + datetime.timedelta(days=i)\n",
    "    data.append((i, cat, amt, dt))\n",
    "\n",
    "# 3. Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# 4. Add random flag column\n",
    "df = df.withColumn(\"flag\", when(rand() > 0.5, lit(\"Y\")).otherwise(lit(\"N\")))\n",
    "\n",
    "# 5. Extract year, month, day\n",
    "df = df.withColumn(\"year\", year(col(\"date\"))) \\\n",
    "       .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "       .withColumn(\"day\", dayofmonth(col(\"date\")))\n",
    "\n",
    "# 6. Filter for category A or B and flag Y\n",
    "filtered_df = df.filter((col(\"category\").isin(\"A\", \"B\")) & (col(\"flag\") == \"Y\"))\n",
    "\n",
    "# 7. Group by category and month, aggregate\n",
    "agg_df = filtered_df.groupBy(\"category\", \"month\").agg(\n",
    "    count(\"*\").alias(\"txn_count\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\"),\n",
    "    spark_max(\"amount\").alias(\"max_amount\"),\n",
    "    spark_min(\"amount\").alias(\"min_amount\")\n",
    ")\n",
    "\n",
    "# Alias DataFrames\n",
    "agg_df_alias = agg_df.alias(\"agg\")\n",
    "filtered_df_alias = filtered_df.alias(\"filtered\")\n",
    "\n",
    "# 8. Join with original to get detailed info for max transactions\n",
    "max_txn_df = agg_df_alias.join(\n",
    "    filtered_df_alias,\n",
    "    (agg_df_alias.category == filtered_df_alias.category) &\n",
    "    (agg_df_alias.month == filtered_df_alias.month) &\n",
    "    (agg_df_alias.max_amount == filtered_df_alias.amount),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    filtered_df_alias[\"id\"], filtered_df_alias[\"category\"], filtered_df_alias[\"amount\"],\n",
    "    filtered_df_alias[\"date\"], filtered_df_alias[\"month\"], agg_df_alias[\"txn_count\"]\n",
    ")\n",
    "\n",
    "# 9. Sort by month and amount descending\n",
    "result_df = max_txn_df.orderBy(col(\"month\"), col(\"amount\").desc())\n",
    "\n",
    "# 10. Display result\n",
    "display(result_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "max_transactions_in_month",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
